
==> Audit <==
|---------|------------------------------|----------|----------------------|---------|----------------------|----------------------|
| Command |             Args             | Profile  |         User         | Version |      Start Time      |       End Time       |
|---------|------------------------------|----------|----------------------|---------|----------------------|----------------------|
| start   |                              | minikube | DESKTOP-HN5PBQL\Omar | v1.33.0 | 22 May 24 13:36 EEST | 22 May 24 13:57 EEST |
| service | springboot-app-service --url | minikube | DESKTOP-HN5PBQL\Omar | v1.33.0 | 22 May 24 14:27 EEST |                      |
| service | springboot-app-service --url | minikube | DESKTOP-HN5PBQL\Omar | v1.33.0 | 22 May 24 14:29 EEST |                      |
|---------|------------------------------|----------|----------------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/05/22 13:36:17
Running on machine: DESKTOP-HN5PBQL
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0522 13:36:17.039540    7692 out.go:291] Setting OutFile to fd 96 ...
I0522 13:36:17.039540    7692 out.go:304] Setting ErrFile to fd 100...
W0522 13:36:17.054844    7692 root.go:314] Error reading config file at C:\Users\Omar\.minikube\config\config.json: open C:\Users\Omar\.minikube\config\config.json: The system cannot find the path specified.
I0522 13:36:17.204023    7692 out.go:298] Setting JSON to false
I0522 13:36:17.212102    7692 start.go:129] hostinfo: {"hostname":"DESKTOP-HN5PBQL","uptime":210165,"bootTime":1716164011,"procs":263,"os":"windows","platform":"Microsoft Windows 11 Pro N for Workstations","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.3593 Build 22621.3593","kernelVersion":"10.0.22621.3593 Build 22621.3593","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"6919fda3-550d-4c2b-9a32-7c0bb5f210bb"}
W0522 13:36:17.212230    7692 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0522 13:36:17.296672    7692 out.go:177] 😄  minikube v1.33.0 on Microsoft Windows 11 Pro N For Workstations 10.0.22621.3593 Build 22621.3593
I0522 13:36:17.366832    7692 notify.go:220] Checking for updates...
W0522 13:36:17.367578    7692 preload.go:294] Failed to list preload files: open C:\Users\Omar\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I0522 13:36:17.370594    7692 driver.go:392] Setting default libvirt URI to qemu:///system
I0522 13:36:17.371168    7692 global.go:112] Querying for installed drivers using PATH=C:\Program Files\Java\jdk-21\bin;C:\Program Files\Common Files\Oracle\Java\javapath;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\dotnet\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\nodejs\;C:\Program Files\Git\cmd;C:\Program Files\apache-maven-3.9.6\bin;C:\Program Files\PuTTY\;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Kubernetes\Minikube;C:\Users\Omar\AppData\Local\Programs\Python\Python312\Scripts\;C:\Users\Omar\AppData\Local\Programs\Python\Python312\;C:\Users\Omar\AppData\Local\Programs\Python\Launcher\;C:\Users\Omar\flutter_windows_3.16.5-stable;C:\Users\Omar\AppData\Local\Microsoft\WindowsApps;C:\Users\Omar\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\Omar\AppData\Roaming\npm;C:\Users\Omar\.dotnet\tools;C:\Users\Omar\AppData\Local\Programs\Python\Python312\Scripts\uvicorn.exe;
I0522 13:36:17.813351    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\last_update_check: {Name:mk8aa8c85050e118d0be1c4b970d5f2cf29ab9fb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:36:17.892517    7692 out.go:177] 🎉  minikube 1.33.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.33.1
I0522 13:36:17.939501    7692 out.go:177] 💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0522 13:36:18.229775    7692 virtualbox.go:136] virtual box version: 7.0.14r161095
I0522 13:36:18.229867    7692 global.go:133] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:7.0.14r161095
}
I0522 13:36:18.241253    7692 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0522 13:36:18.397190    7692 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I0522 13:36:18.402723    7692 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0522 13:36:24.077412    7692 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (5.6746887s)
I0522 13:36:24.077412    7692 info.go:266] docker info: {ID:bdc7d674-0a9d-4d63-a615-6e9a22f7fad0 Containers:29 ContainersRunning:0 ContainersPaused:0 ContainersStopped:29 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:68 OomKillDisable:true NGoroutines:106 SystemTime:2024-05-22 10:14:43.89684553 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:26 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8208326656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0522 13:36:24.077985    7692 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0522 13:36:24.092798    7692 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0522 13:36:24.092798    7692 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0522 13:36:30.185974    7692 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:Hyper-V requires Administrator privileges Reason: Fix:Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode. Doc: Version:}
I0522 13:36:30.202030    7692 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0522 13:36:30.202030    7692 driver.go:314] not recommending "ssh" due to default: false
I0522 13:36:30.202538    7692 driver.go:309] not recommending "hyperv" due to health: Hyper-V requires Administrator privileges
I0522 13:36:30.202538    7692 driver.go:349] Picked: docker
I0522 13:36:30.202538    7692 driver.go:350] Alternatives: [virtualbox ssh]
I0522 13:36:30.202538    7692 driver.go:351] Rejects: [vmware podman hyperv qemu2]
I0522 13:36:30.258619    7692 out.go:177] ✨  Automatically selected the docker driver. Other choices: virtualbox, ssh
I0522 13:36:30.284626    7692 start.go:297] selected driver: docker
I0522 13:36:30.284626    7692 start.go:901] validating driver "docker" against <nil>
I0522 13:36:30.284699    7692 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0522 13:36:30.302624    7692 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0522 13:36:30.623395    7692 info.go:266] docker info: {ID:bdc7d674-0a9d-4d63-a615-6e9a22f7fad0 Containers:29 ContainersRunning:0 ContainersPaused:0 ContainersStopped:29 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:68 OomKillDisable:true NGoroutines:106 SystemTime:2024-05-22 10:14:43.89684553 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:26 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8208326656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0522 13:36:30.623911    7692 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0522 13:36:30.701579    7692 start_flags.go:393] Using suggested 4000MB memory alloc based on sys=16151MB, container=7828MB
I0522 13:36:30.702120    7692 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0522 13:36:30.743889    7692 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0522 13:36:30.791963    7692 cni.go:84] Creating CNI manager for ""
I0522 13:36:30.791963    7692 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0522 13:36:30.791963    7692 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0522 13:36:30.791963    7692 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Omar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0522 13:36:30.826813    7692 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0522 13:36:30.877328    7692 cache.go:121] Beginning downloading kic base image for docker with docker
I0522 13:36:30.900909    7692 out.go:177] 🚜  Pulling base image v0.0.43 ...
I0522 13:36:30.926516    7692 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0522 13:36:30.926516    7692 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0522 13:36:31.069232    7692 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0522 13:36:31.069776    7692 localpath.go:146] windows sanitize: C:\Users\Omar\.minikube\cache\kic\amd64\kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar -> C:\Users\Omar\.minikube\cache\kic\amd64\kicbase_v0.0.43@sha256_7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar
I0522 13:36:31.069776    7692 localpath.go:146] windows sanitize: C:\Users\Omar\.minikube\cache\kic\amd64\kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar -> C:\Users\Omar\.minikube\cache\kic\amd64\kicbase_v0.0.43@sha256_7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar
I0522 13:36:31.070312    7692 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local cache directory
I0522 13:36:31.070849    7692 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0522 13:36:31.107167    7692 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0522 13:36:31.107167    7692 cache.go:56] Caching tarball of preloaded images
I0522 13:36:31.107167    7692 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0522 13:36:31.132695    7692 out.go:177] 💾  Downloading Kubernetes v1.30.0 preload ...
I0522 13:36:31.157311    7692 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0522 13:36:31.523675    7692 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4?checksum=md5:00b6acf85a82438f3897c0a6fafdcee7 -> C:\Users\Omar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0522 13:48:27.181141    7692 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0522 13:48:27.321462    7692 preload.go:255] verifying checksum of C:\Users\Omar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0522 13:48:28.260559    7692 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0522 13:48:28.260559    7692 profile.go:143] Saving config to C:\Users\Omar\.minikube\profiles\minikube\config.json ...
I0522 13:48:28.260559    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\profiles\minikube\config.json: {Name:mk425e98b3f1bd83efdc16b61386879b0c8e1528 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:51:52.251773    7692 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 as a tarball
I0522 13:51:52.251773    7692 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from local cache
I0522 13:51:52.252281    7692 localpath.go:146] windows sanitize: C:\Users\Omar\.minikube\cache\kic\amd64\kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar -> C:\Users\Omar\.minikube\cache\kic\amd64\kicbase_v0.0.43@sha256_7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar
I0522 13:53:56.458000    7692 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from cached tarball
I0522 13:53:56.458000    7692 cache.go:194] Successfully downloaded all kic artifacts
I0522 13:53:56.458550    7692 start.go:360] acquireMachinesLock for minikube: {Name:mka9274f358a961d8b18bff6e4bd30ed70318875 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0522 13:53:56.458550    7692 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0522 13:53:56.458550    7692 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Omar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0522 13:53:56.459065    7692 start.go:125] createHost starting for "" (driver="docker")
I0522 13:53:56.597253    7692 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=4000MB) ...
I0522 13:53:56.597758    7692 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0522 13:53:56.598293    7692 client.go:168] LocalClient.Create starting
I0522 13:53:56.598832    7692 main.go:141] libmachine: Creating CA: C:\Users\Omar\.minikube\certs\ca.pem
I0522 13:53:56.692935    7692 main.go:141] libmachine: Creating client certificate: C:\Users\Omar\.minikube\certs\cert.pem
I0522 13:53:57.239668    7692 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0522 13:53:57.474443    7692 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0522 13:53:57.481389    7692 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0522 13:53:57.481389    7692 cli_runner.go:164] Run: docker network inspect minikube
W0522 13:53:57.597054    7692 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0522 13:53:57.597054    7692 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0522 13:53:57.597054    7692 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0522 13:53:57.606123    7692 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0522 13:53:57.772697    7692 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc00130d560}
I0522 13:53:57.772697    7692 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0522 13:53:57.779556    7692 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0522 13:53:58.889158    7692 cli_runner.go:217] Completed: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube: (1.1096019s)
I0522 13:53:58.889158    7692 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0522 13:53:58.889158    7692 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0522 13:53:58.901637    7692 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0522 13:53:59.310321    7692 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0522 13:54:01.401947    7692 cli_runner.go:217] Completed: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true: (2.0916262s)
I0522 13:54:01.401947    7692 oci.go:103] Successfully created a docker volume minikube
I0522 13:54:01.408270    7692 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0522 13:54:36.337609    7692 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib: (34.9293386s)
I0522 13:54:36.337609    7692 oci.go:107] Successfully prepared a docker volume minikube
I0522 13:54:36.337609    7692 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0522 13:54:36.337609    7692 kic.go:194] Starting extracting preloaded images to volume ...
I0522 13:54:36.348008    7692 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Omar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0522 13:55:32.981508    7692 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Omar\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (56.6335003s)
I0522 13:55:32.981508    7692 kic.go:203] duration metric: took 56.6438995s to extract preloaded images to volume ...
I0522 13:55:32.989550    7692 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0522 13:55:34.154466    7692 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.1649166s)
I0522 13:55:34.154466    7692 info.go:266] docker info: {ID:bdc7d674-0a9d-4d63-a615-6e9a22f7fad0 Containers:29 ContainersRunning:0 ContainersPaused:0 ContainersStopped:29 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:69 OomKillDisable:true NGoroutines:108 SystemTime:2024-05-22 10:55:34.111551385 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:27 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8208326656 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0522 13:55:34.162793    7692 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0522 13:55:34.800692    7692 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0522 13:55:40.422183    7692 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737: (5.6214914s)
I0522 13:55:40.430147    7692 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0522 13:55:40.586578    7692 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0522 13:55:40.722485    7692 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0522 13:55:40.992338    7692 oci.go:144] the created container "minikube" has a running status.
I0522 13:55:40.992338    7692 kic.go:225] Creating ssh key for kic: C:\Users\Omar\.minikube\machines\minikube\id_rsa...
I0522 13:55:41.335867    7692 kic_runner.go:191] docker (temp): C:\Users\Omar\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0522 13:55:41.637783    7692 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0522 13:55:41.803117    7692 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0522 13:55:41.803117    7692 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0522 13:55:41.986617    7692 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Omar\.minikube\machines\minikube\id_rsa...
I0522 13:55:44.603928    7692 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0522 13:55:44.728969    7692 machine.go:94] provisionDockerMachine start ...
I0522 13:55:44.736806    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:44.934270    7692 main.go:141] libmachine: Using SSH client type: native
I0522 13:55:44.941520    7692 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf0a1c0] 0xf0cda0 <nil>  [] 0s} 127.0.0.1 51675 <nil> <nil>}
I0522 13:55:44.941520    7692 main.go:141] libmachine: About to run SSH command:
hostname
I0522 13:55:45.102726    7692 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0522 13:55:45.102726    7692 ubuntu.go:169] provisioning hostname "minikube"
I0522 13:55:45.112244    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:45.238892    7692 main.go:141] libmachine: Using SSH client type: native
I0522 13:55:45.239396    7692 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf0a1c0] 0xf0cda0 <nil>  [] 0s} 127.0.0.1 51675 <nil> <nil>}
I0522 13:55:45.239396    7692 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0522 13:55:45.444197    7692 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0522 13:55:45.454818    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:45.595051    7692 main.go:141] libmachine: Using SSH client type: native
I0522 13:55:45.595559    7692 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf0a1c0] 0xf0cda0 <nil>  [] 0s} 127.0.0.1 51675 <nil> <nil>}
I0522 13:55:45.595559    7692 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0522 13:55:45.795463    7692 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0522 13:55:45.796001    7692 ubuntu.go:175] set auth options {CertDir:C:\Users\Omar\.minikube CaCertPath:C:\Users\Omar\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Omar\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Omar\.minikube\machines\server.pem ServerKeyPath:C:\Users\Omar\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Omar\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Omar\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Omar\.minikube}
I0522 13:55:45.796070    7692 ubuntu.go:177] setting up certificates
I0522 13:55:45.796070    7692 provision.go:84] configureAuth start
I0522 13:55:45.815688    7692 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0522 13:55:45.961407    7692 provision.go:143] copyHostCerts
I0522 13:55:45.961962    7692 exec_runner.go:151] cp: C:\Users\Omar\.minikube\certs\ca.pem --> C:\Users\Omar\.minikube/ca.pem (1070 bytes)
I0522 13:55:45.963607    7692 exec_runner.go:151] cp: C:\Users\Omar\.minikube\certs\cert.pem --> C:\Users\Omar\.minikube/cert.pem (1115 bytes)
I0522 13:55:45.965596    7692 exec_runner.go:151] cp: C:\Users\Omar\.minikube\certs\key.pem --> C:\Users\Omar\.minikube/key.pem (1675 bytes)
I0522 13:55:45.966159    7692 provision.go:117] generating server cert: C:\Users\Omar\.minikube\machines\server.pem ca-key=C:\Users\Omar\.minikube\certs\ca.pem private-key=C:\Users\Omar\.minikube\certs\ca-key.pem org=Omar.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0522 13:55:46.200140    7692 provision.go:177] copyRemoteCerts
I0522 13:55:46.213669    7692 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0522 13:55:46.220600    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:46.352785    7692 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51675 SSHKeyPath:C:\Users\Omar\.minikube\machines\minikube\id_rsa Username:docker}
I0522 13:55:46.461684    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0522 13:55:46.482924    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0522 13:55:46.502673    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0522 13:55:46.522885    7692 provision.go:87] duration metric: took 726.8153ms to configureAuth
I0522 13:55:46.522885    7692 ubuntu.go:193] setting minikube options for container-runtime
I0522 13:55:46.523416    7692 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0522 13:55:46.529520    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:46.697340    7692 main.go:141] libmachine: Using SSH client type: native
I0522 13:55:46.697432    7692 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf0a1c0] 0xf0cda0 <nil>  [] 0s} 127.0.0.1 51675 <nil> <nil>}
I0522 13:55:46.697432    7692 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0522 13:55:46.827153    7692 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0522 13:55:46.827153    7692 ubuntu.go:71] root file system type: overlay
I0522 13:55:46.827153    7692 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0522 13:55:46.834949    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:46.986064    7692 main.go:141] libmachine: Using SSH client type: native
I0522 13:55:46.986596    7692 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf0a1c0] 0xf0cda0 <nil>  [] 0s} 127.0.0.1 51675 <nil> <nil>}
I0522 13:55:46.986596    7692 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0522 13:55:47.129893    7692 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0522 13:55:47.213244    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:47.372779    7692 main.go:141] libmachine: Using SSH client type: native
I0522 13:55:47.373340    7692 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xf0a1c0] 0xf0cda0 <nil>  [] 0s} 127.0.0.1 51675 <nil> <nil>}
I0522 13:55:47.373340    7692 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0522 13:55:59.617436    7692 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-22 10:55:47.121347939 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0522 13:55:59.617436    7692 machine.go:97] duration metric: took 14.8884671s to provisionDockerMachine
I0522 13:55:59.617436    7692 client.go:171] duration metric: took 2m3.0191432s to LocalClient.Create
I0522 13:55:59.617436    7692 start.go:167] duration metric: took 2m3.019678s to libmachine.API.Create "minikube"
I0522 13:55:59.617436    7692 start.go:293] postStartSetup for "minikube" (driver="docker")
I0522 13:55:59.617436    7692 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0522 13:55:59.627966    7692 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0522 13:55:59.634433    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:55:59.758333    7692 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51675 SSHKeyPath:C:\Users\Omar\.minikube\machines\minikube\id_rsa Username:docker}
I0522 13:55:59.911885    7692 ssh_runner.go:195] Run: cat /etc/os-release
I0522 13:55:59.916772    7692 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0522 13:55:59.916772    7692 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0522 13:55:59.916772    7692 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0522 13:55:59.916772    7692 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0522 13:55:59.917299    7692 filesync.go:126] Scanning C:\Users\Omar\.minikube\addons for local assets ...
I0522 13:55:59.917299    7692 filesync.go:126] Scanning C:\Users\Omar\.minikube\files for local assets ...
I0522 13:55:59.917861    7692 start.go:296] duration metric: took 300.4245ms for postStartSetup
I0522 13:55:59.925475    7692 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0522 13:56:00.051625    7692 profile.go:143] Saving config to C:\Users\Omar\.minikube\profiles\minikube\config.json ...
I0522 13:56:00.083756    7692 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0522 13:56:00.089079    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:56:00.297190    7692 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51675 SSHKeyPath:C:\Users\Omar\.minikube\machines\minikube\id_rsa Username:docker}
I0522 13:56:00.409622    7692 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0522 13:56:00.415604    7692 start.go:128] duration metric: took 2m3.9565384s to createHost
I0522 13:56:00.415604    7692 start.go:83] releasing machines lock for "minikube", held for 2m3.9570537s
I0522 13:56:00.421631    7692 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0522 13:56:00.590734    7692 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0522 13:56:00.599343    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:56:00.602988    7692 ssh_runner.go:195] Run: cat /version.json
I0522 13:56:00.610292    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:56:00.741734    7692 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51675 SSHKeyPath:C:\Users\Omar\.minikube\machines\minikube\id_rsa Username:docker}
I0522 13:56:00.758953    7692 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51675 SSHKeyPath:C:\Users\Omar\.minikube\machines\minikube\id_rsa Username:docker}
I0522 13:56:01.275183    7692 ssh_runner.go:195] Run: systemctl --version
I0522 13:56:01.293280    7692 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0522 13:56:01.309114    7692 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0522 13:56:01.319025    7692 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0522 13:56:01.332819    7692 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0522 13:56:01.737567    7692 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0522 13:56:01.737567    7692 start.go:494] detecting cgroup driver to use...
I0522 13:56:01.737641    7692 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0522 13:56:01.737641    7692 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0522 13:56:01.769756    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0522 13:56:01.790998    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0522 13:56:01.801460    7692 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0522 13:56:01.810694    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0522 13:56:01.958521    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0522 13:56:01.979762    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0522 13:56:01.998671    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0522 13:56:02.017194    7692 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0522 13:56:02.038382    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0522 13:56:02.060630    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0522 13:56:02.141813    7692 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0522 13:56:02.164374    7692 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0522 13:56:02.184575    7692 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0522 13:56:02.203127    7692 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0522 13:56:02.314609    7692 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0522 13:56:02.458858    7692 start.go:494] detecting cgroup driver to use...
I0522 13:56:02.458858    7692 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0522 13:56:02.475377    7692 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0522 13:56:02.489095    7692 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0522 13:56:02.501013    7692 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0522 13:56:02.514753    7692 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0522 13:56:02.540726    7692 ssh_runner.go:195] Run: which cri-dockerd
I0522 13:56:02.555595    7692 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0522 13:56:02.564665    7692 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0522 13:56:02.593189    7692 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0522 13:56:02.690914    7692 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0522 13:56:02.807900    7692 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0522 13:56:02.808665    7692 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0522 13:56:02.835980    7692 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0522 13:56:02.944584    7692 ssh_runner.go:195] Run: sudo systemctl restart docker
I0522 13:56:07.893057    7692 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.9484728s)
I0522 13:56:07.903802    7692 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0522 13:56:07.930116    7692 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0522 13:56:07.952839    7692 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0522 13:56:08.069403    7692 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0522 13:56:08.182983    7692 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0522 13:56:08.291649    7692 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0522 13:56:08.313662    7692 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0522 13:56:08.335999    7692 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0522 13:56:08.437016    7692 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0522 13:56:08.536924    7692 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0522 13:56:08.548505    7692 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0522 13:56:08.552892    7692 start.go:562] Will wait 60s for crictl version
I0522 13:56:08.563094    7692 ssh_runner.go:195] Run: which crictl
I0522 13:56:08.578447    7692 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0522 13:56:08.617652    7692 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0522 13:56:08.625398    7692 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0522 13:56:08.662165    7692 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0522 13:56:08.767155    7692 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0522 13:56:08.778992    7692 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0522 13:56:09.327573    7692 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0522 13:56:09.341200    7692 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0522 13:56:09.347081    7692 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0522 13:56:09.365306    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0522 13:56:09.479871    7692 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Omar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0522 13:56:09.479871    7692 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0522 13:56:09.486736    7692 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0522 13:56:09.506020    7692 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0522 13:56:09.506020    7692 docker.go:615] Images already preloaded, skipping extraction
I0522 13:56:09.512561    7692 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0522 13:56:09.534103    7692 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0522 13:56:09.534103    7692 cache_images.go:84] Images are preloaded, skipping loading
I0522 13:56:09.534103    7692 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0522 13:56:09.534103    7692 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0522 13:56:09.543895    7692 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0522 13:56:09.603775    7692 cni.go:84] Creating CNI manager for ""
I0522 13:56:09.603775    7692 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0522 13:56:09.603775    7692 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0522 13:56:09.603775    7692 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0522 13:56:09.603775    7692 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0522 13:56:09.618667    7692 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0522 13:56:09.629168    7692 binaries.go:44] Found k8s binaries, skipping transfer
I0522 13:56:09.640396    7692 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0522 13:56:09.649916    7692 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0522 13:56:09.667893    7692 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0522 13:56:09.685188    7692 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0522 13:56:09.715779    7692 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0522 13:56:09.721395    7692 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0522 13:56:09.745840    7692 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0522 13:56:09.858379    7692 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0522 13:56:09.871294    7692 certs.go:68] Setting up C:\Users\Omar\.minikube\profiles\minikube for IP: 192.168.49.2
I0522 13:56:09.871294    7692 certs.go:194] generating shared ca certs ...
I0522 13:56:09.871830    7692 certs.go:226] acquiring lock for ca certs: {Name:mk7507cf646d2ab1bac3f86808e5f5f382f28818 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:09.871830    7692 certs.go:240] generating "minikubeCA" ca cert: C:\Users\Omar\.minikube\ca.key
I0522 13:56:09.980482    7692 crypto.go:156] Writing cert to C:\Users\Omar\.minikube\ca.crt ...
I0522 13:56:09.980482    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\ca.crt: {Name:mk176f3b17e5841d9950215d31b2e6251453e90a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:09.981468    7692 crypto.go:164] Writing key to C:\Users\Omar\.minikube\ca.key ...
I0522 13:56:09.981468    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\ca.key: {Name:mk81bde18df44a7a2f6ae9f0b36e227f35e45ef5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:09.982471    7692 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\Omar\.minikube\proxy-client-ca.key
I0522 13:56:10.324705    7692 crypto.go:156] Writing cert to C:\Users\Omar\.minikube\proxy-client-ca.crt ...
I0522 13:56:10.324705    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\proxy-client-ca.crt: {Name:mkdd8ab78db7a8a2c04136a420622ef54e588043 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:10.325921    7692 crypto.go:164] Writing key to C:\Users\Omar\.minikube\proxy-client-ca.key ...
I0522 13:56:10.325921    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\proxy-client-ca.key: {Name:mk687f0d4070e560f2e7dfd6e0d0c4f9bf7c1954 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:10.326925    7692 certs.go:256] generating profile certs ...
I0522 13:56:10.327917    7692 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Omar\.minikube\profiles\minikube\client.key
I0522 13:56:10.327917    7692 crypto.go:68] Generating cert C:\Users\Omar\.minikube\profiles\minikube\client.crt with IP's: []
I0522 13:56:10.510886    7692 crypto.go:156] Writing cert to C:\Users\Omar\.minikube\profiles\minikube\client.crt ...
I0522 13:56:10.510886    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\profiles\minikube\client.crt: {Name:mkc7715f3e6da5b713d464d051d289b67653c326 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:10.511887    7692 crypto.go:164] Writing key to C:\Users\Omar\.minikube\profiles\minikube\client.key ...
I0522 13:56:10.511887    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\profiles\minikube\client.key: {Name:mk54ea28c2d2331bac8c68c0c5dd7c99518940d1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:10.511887    7692 certs.go:363] generating signed profile cert for "minikube": C:\Users\Omar\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0522 13:56:10.511887    7692 crypto.go:68] Generating cert C:\Users\Omar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0522 13:56:10.854831    7692 crypto.go:156] Writing cert to C:\Users\Omar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0522 13:56:10.854831    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkb791ef164ca2a1c87dac6dcb1d1c95a1bb6d41 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:10.919387    7692 crypto.go:164] Writing key to C:\Users\Omar\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0522 13:56:10.919387    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mkcaf0f2a830e0bf2481c0a33ca8caf0200ee615 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:10.930428    7692 certs.go:381] copying C:\Users\Omar\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Omar\.minikube\profiles\minikube\apiserver.crt
I0522 13:56:10.957868    7692 certs.go:385] copying C:\Users\Omar\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Omar\.minikube\profiles\minikube\apiserver.key
I0522 13:56:10.983220    7692 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Omar\.minikube\profiles\minikube\proxy-client.key
I0522 13:56:10.983220    7692 crypto.go:68] Generating cert C:\Users\Omar\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0522 13:56:11.170665    7692 crypto.go:156] Writing cert to C:\Users\Omar\.minikube\profiles\minikube\proxy-client.crt ...
I0522 13:56:11.170665    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\profiles\minikube\proxy-client.crt: {Name:mke60296d37b7edd94f936f2d6524f0b613dd0c2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:11.171666    7692 crypto.go:164] Writing key to C:\Users\Omar\.minikube\profiles\minikube\proxy-client.key ...
I0522 13:56:11.171666    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.minikube\profiles\minikube\proxy-client.key: {Name:mk0f85555b0adc7db5a817387cbdf52484a3f9fc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:56:11.216339    7692 certs.go:484] found cert: C:\Users\Omar\.minikube\certs\ca-key.pem (1679 bytes)
I0522 13:56:11.216339    7692 certs.go:484] found cert: C:\Users\Omar\.minikube\certs\ca.pem (1070 bytes)
I0522 13:56:11.216339    7692 certs.go:484] found cert: C:\Users\Omar\.minikube\certs\cert.pem (1115 bytes)
I0522 13:56:11.216846    7692 certs.go:484] found cert: C:\Users\Omar\.minikube\certs\key.pem (1675 bytes)
I0522 13:56:11.218040    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0522 13:56:11.242722    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0522 13:56:11.263077    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0522 13:56:11.282618    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0522 13:56:11.305403    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0522 13:56:11.328874    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0522 13:56:11.354238    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0522 13:56:11.380601    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0522 13:56:11.405508    7692 ssh_runner.go:362] scp C:\Users\Omar\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0522 13:56:11.426404    7692 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0522 13:56:11.456686    7692 ssh_runner.go:195] Run: openssl version
I0522 13:56:11.472797    7692 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0522 13:56:11.495797    7692 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0522 13:56:11.502175    7692 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 22 10:56 /usr/share/ca-certificates/minikubeCA.pem
I0522 13:56:11.520940    7692 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0522 13:56:11.542442    7692 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0522 13:56:11.563054    7692 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0522 13:56:11.567295    7692 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0522 13:56:11.567822    7692 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Omar:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0522 13:56:11.573887    7692 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0522 13:56:11.599564    7692 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0522 13:56:11.619559    7692 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0522 13:56:11.629582    7692 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0522 13:56:11.643897    7692 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0522 13:56:11.654443    7692 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0522 13:56:11.654443    7692 kubeadm.go:156] found existing configuration files:

I0522 13:56:11.665663    7692 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0522 13:56:11.674259    7692 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0522 13:56:11.686370    7692 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0522 13:56:11.706700    7692 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0522 13:56:11.714858    7692 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0522 13:56:11.724542    7692 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0522 13:56:11.747813    7692 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0522 13:56:11.757598    7692 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0522 13:56:11.768331    7692 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0522 13:56:11.786483    7692 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0522 13:56:11.794658    7692 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0522 13:56:11.808560    7692 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0522 13:56:11.816846    7692 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0522 13:56:11.912089    7692 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0522 13:56:11.978393    7692 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0522 13:57:30.573573    7692 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0522 13:57:30.573573    7692 kubeadm.go:309] [preflight] Running pre-flight checks
I0522 13:57:30.573800    7692 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0522 13:57:30.573800    7692 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0522 13:57:30.573800    7692 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0522 13:57:30.573800    7692 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0522 13:57:30.793452    7692 out.go:204]     ▪ Generating certificates and keys ...
I0522 13:57:30.796053    7692 kubeadm.go:309] [certs] Using existing ca certificate authority
I0522 13:57:30.796053    7692 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0522 13:57:30.796053    7692 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0522 13:57:30.796053    7692 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0522 13:57:30.796659    7692 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0522 13:57:30.796849    7692 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0522 13:57:30.796900    7692 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0522 13:57:30.797410    7692 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0522 13:57:30.797536    7692 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0522 13:57:30.798052    7692 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0522 13:57:30.798052    7692 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0522 13:57:30.798052    7692 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0522 13:57:30.798052    7692 kubeadm.go:309] [certs] Generating "sa" key and public key
I0522 13:57:30.798593    7692 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0522 13:57:30.798593    7692 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0522 13:57:30.798593    7692 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0522 13:57:30.798593    7692 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0522 13:57:30.798593    7692 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0522 13:57:30.799134    7692 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0522 13:57:30.799209    7692 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0522 13:57:30.799209    7692 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0522 13:57:31.009125    7692 out.go:204]     ▪ Booting up control plane ...
I0522 13:57:31.010929    7692 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0522 13:57:31.010929    7692 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0522 13:57:31.010929    7692 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0522 13:57:31.010929    7692 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0522 13:57:31.010929    7692 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0522 13:57:31.011505    7692 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0522 13:57:31.011505    7692 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0522 13:57:31.011505    7692 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0522 13:57:31.011505    7692 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 1.002172978s
I0522 13:57:31.012035    7692 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0522 13:57:31.012141    7692 kubeadm.go:309] [api-check] The API server is healthy after 1m1.501461713s
I0522 13:57:31.012141    7692 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0522 13:57:31.012141    7692 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0522 13:57:31.012141    7692 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0522 13:57:31.012845    7692 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0522 13:57:31.012845    7692 kubeadm.go:309] [bootstrap-token] Using token: cqainl.x1eucsnhqqqxlwas
I0522 13:57:31.274421    7692 out.go:204]     ▪ Configuring RBAC rules ...
I0522 13:57:31.276213    7692 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0522 13:57:31.276213    7692 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0522 13:57:31.276836    7692 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0522 13:57:31.276992    7692 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0522 13:57:31.276992    7692 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0522 13:57:31.276992    7692 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0522 13:57:31.277616    7692 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0522 13:57:31.277738    7692 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0522 13:57:31.277831    7692 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0522 13:57:31.277831    7692 kubeadm.go:309] 
I0522 13:57:31.277831    7692 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0522 13:57:31.277831    7692 kubeadm.go:309] 
I0522 13:57:31.277831    7692 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0522 13:57:31.277831    7692 kubeadm.go:309] 
I0522 13:57:31.277831    7692 kubeadm.go:309]   mkdir -p $HOME/.kube
I0522 13:57:31.277831    7692 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0522 13:57:31.277831    7692 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0522 13:57:31.277831    7692 kubeadm.go:309] 
I0522 13:57:31.277831    7692 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0522 13:57:31.277831    7692 kubeadm.go:309] 
I0522 13:57:31.277831    7692 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0522 13:57:31.277831    7692 kubeadm.go:309] 
I0522 13:57:31.277831    7692 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0522 13:57:31.278338    7692 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0522 13:57:31.278338    7692 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0522 13:57:31.278338    7692 kubeadm.go:309] 
I0522 13:57:31.278338    7692 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0522 13:57:31.278338    7692 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0522 13:57:31.278338    7692 kubeadm.go:309] 
I0522 13:57:31.278338    7692 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token cqainl.x1eucsnhqqqxlwas \
I0522 13:57:31.278879    7692 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:16e2b942c6018e735aeee43dc28db4a9dd1519bb7d5a72db0c166c36ca5d0059 \
I0522 13:57:31.278879    7692 kubeadm.go:309] 	--control-plane 
I0522 13:57:31.278879    7692 kubeadm.go:309] 
I0522 13:57:31.278940    7692 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0522 13:57:31.278940    7692 kubeadm.go:309] 
I0522 13:57:31.278940    7692 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token cqainl.x1eucsnhqqqxlwas \
I0522 13:57:31.278940    7692 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:16e2b942c6018e735aeee43dc28db4a9dd1519bb7d5a72db0c166c36ca5d0059 
I0522 13:57:31.278940    7692 cni.go:84] Creating CNI manager for ""
I0522 13:57:31.279481    7692 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0522 13:57:31.414701    7692 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0522 13:57:31.633542    7692 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0522 13:57:31.647136    7692 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0522 13:57:31.663994    7692 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0522 13:57:31.675657    7692 ops.go:34] apiserver oom_adj: -16
I0522 13:57:31.682783    7692 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0522 13:57:31.683313    7692 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_22T13_57_31_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0522 13:57:32.346084    7692 kubeadm.go:1107] duration metric: took 682.0899ms to wait for elevateKubeSystemPrivileges
W0522 13:57:32.577486    7692 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0522 13:57:32.577486    7692 kubeadm.go:393] duration metric: took 1m21.0096633s to StartCluster
I0522 13:57:32.577486    7692 settings.go:142] acquiring lock: {Name:mk4955e5777ab78b48abf37274a7f593416a1ac4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:57:32.577486    7692 settings.go:150] Updating kubeconfig:  C:\Users\Omar\.kube\config
I0522 13:57:32.578619    7692 lock.go:35] WriteFile acquiring C:\Users\Omar\.kube\config: {Name:mk9c5d642fba86d4482ea4c26ba106e045d1000f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0522 13:57:32.579661    7692 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0522 13:57:32.579661    7692 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0522 13:57:32.579661    7692 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0522 13:57:32.579661    7692 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0522 13:57:32.579661    7692 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0522 13:57:32.651296    7692 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0522 13:57:32.651296    7692 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0522 13:57:32.651296    7692 out.go:177] 🔎  Verifying Kubernetes components...
I0522 13:57:32.579661    7692 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0522 13:57:32.651760    7692 host.go:66] Checking if "minikube" exists ...
I0522 13:57:32.668337    7692 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0522 13:57:32.669490    7692 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0522 13:57:32.824339    7692 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0522 13:57:32.827313    7692 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0522 13:57:32.985904    7692 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0522 13:57:32.985904    7692 host.go:66] Checking if "minikube" exists ...
I0522 13:57:33.010429    7692 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0522 13:57:33.142172    7692 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0522 13:57:33.187894    7692 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0522 13:57:33.187894    7692 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0522 13:57:33.193355    7692 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0522 13:57:33.198150    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:57:33.208971    7692 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0522 13:57:33.238943    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0522 13:57:33.307856    7692 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0522 13:57:33.307856    7692 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0522 13:57:33.319985    7692 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0522 13:57:33.381790    7692 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51675 SSHKeyPath:C:\Users\Omar\.minikube\machines\minikube\id_rsa Username:docker}
I0522 13:57:33.414977    7692 api_server.go:52] waiting for apiserver process to appear ...
I0522 13:57:33.430958    7692 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0522 13:57:33.444420    7692 api_server.go:72] duration metric: took 864.759ms to wait for apiserver process to appear ...
I0522 13:57:33.444420    7692 api_server.go:88] waiting for apiserver healthz status ...
I0522 13:57:33.444420    7692 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51674/healthz ...
I0522 13:57:33.452650    7692 api_server.go:279] https://127.0.0.1:51674/healthz returned 200:
ok
I0522 13:57:33.455961    7692 api_server.go:141] control plane version: v1.30.0
I0522 13:57:33.455961    7692 api_server.go:131] duration metric: took 11.5406ms to wait for apiserver health ...
I0522 13:57:33.455961    7692 system_pods.go:43] waiting for kube-system pods to appear ...
I0522 13:57:33.489441    7692 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51675 SSHKeyPath:C:\Users\Omar\.minikube\machines\minikube\id_rsa Username:docker}
I0522 13:57:33.504127    7692 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0522 13:57:33.550383    7692 system_pods.go:59] 4 kube-system pods found
I0522 13:57:33.550383    7692 system_pods.go:61] "etcd-minikube" [fa1510ba-aad5-472c-b45a-7dee9668b443] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0522 13:57:33.550383    7692 system_pods.go:61] "kube-apiserver-minikube" [c111f107-d009-4d9f-bc66-cec63d2eb8ff] Running
I0522 13:57:33.550383    7692 system_pods.go:61] "kube-controller-manager-minikube" [7e625a77-1a0e-40ec-a6bd-77c590d8dffd] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0522 13:57:33.550383    7692 system_pods.go:61] "kube-scheduler-minikube" [4cd58488-638a-4eed-b35a-1a76cb992a3b] Running
I0522 13:57:33.550383    7692 system_pods.go:74] duration metric: took 94.4216ms to wait for pod list to return data ...
I0522 13:57:33.550383    7692 kubeadm.go:576] duration metric: took 970.7212ms to wait for: map[apiserver:true system_pods:true]
I0522 13:57:33.550383    7692 node_conditions.go:102] verifying NodePressure condition ...
I0522 13:57:33.554995    7692 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0522 13:57:33.554995    7692 node_conditions.go:123] node cpu capacity is 8
I0522 13:57:33.554995    7692 node_conditions.go:105] duration metric: took 4.6123ms to run NodePressure ...
I0522 13:57:33.554995    7692 start.go:240] waiting for startup goroutines ...
I0522 13:57:33.628151    7692 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0522 13:57:33.931961    7692 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0522 13:57:37.268336    7692 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.7636924s)
I0522 13:57:37.268336    7692 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.6401847s)
I0522 13:57:39.655231    7692 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0522 13:57:39.747164    7692 addons.go:505] duration metric: took 7.1666971s for enable addons: enabled=[storage-provisioner default-storageclass]
I0522 13:57:39.747164    7692 start.go:245] waiting for cluster config update ...
I0522 13:57:39.747164    7692 start.go:254] writing updated cluster config ...
I0522 13:57:39.764338    7692 ssh_runner.go:195] Run: rm -f paused
I0522 13:57:42.888933    7692 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0522 13:57:43.098257    7692 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 22 10:56:05 minikube dockerd[1000]: time="2024-05-22T10:56:05.398099644Z" level=info msg="API listen on /var/run/docker.sock"
May 22 10:56:05 minikube dockerd[1000]: time="2024-05-22T10:56:05.398157064Z" level=info msg="API listen on [::]:2376"
May 22 10:56:05 minikube dockerd[1000]: time="2024-05-22T10:56:05.399758650Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
May 22 10:56:05 minikube dockerd[1000]: time="2024-05-22T10:56:05.400939040Z" level=info msg="Daemon shutdown complete"
May 22 10:56:05 minikube systemd[1]: docker.service: Deactivated successfully.
May 22 10:56:05 minikube systemd[1]: Stopped Docker Application Container Engine.
May 22 10:56:05 minikube systemd[1]: Starting Docker Application Container Engine...
May 22 10:56:05 minikube dockerd[1221]: time="2024-05-22T10:56:05.453388119Z" level=info msg="Starting up"
May 22 10:56:05 minikube dockerd[1221]: time="2024-05-22T10:56:05.768590713Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 22 10:56:06 minikube dockerd[1221]: time="2024-05-22T10:56:06.182764608Z" level=info msg="Loading containers: start."
May 22 10:56:06 minikube dockerd[1221]: time="2024-05-22T10:56:06.977622712Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.490373390Z" level=info msg="Loading containers: done."
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.675585431Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.675716829Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.675743185Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.675760555Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.675807908Z" level=info msg="Docker daemon" commit=60b9add containerd-snapshotter=false storage-driver=overlay2 version=26.0.1
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.675931106Z" level=info msg="Daemon has completed initialization"
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.890314706Z" level=info msg="API listen on /var/run/docker.sock"
May 22 10:56:07 minikube dockerd[1221]: time="2024-05-22T10:56:07.890347216Z" level=info msg="API listen on [::]:2376"
May 22 10:56:07 minikube systemd[1]: Started Docker Application Container Engine.
May 22 10:56:08 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Start docker client with request timeout 0s"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Loaded network plugin cni"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Docker cri networking managed by network plugin cni"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Setting cgroupDriver cgroupfs"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 22 10:56:08 minikube cri-dockerd[1446]: time="2024-05-22T10:56:08Z" level=info msg="Start cri-dockerd grpc backend"
May 22 10:56:08 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 22 10:56:26 minikube cri-dockerd[1446]: time="2024-05-22T10:56:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/53362bb427ec3f6c5f8f634619c144c85650a5e1c4728967f9ba622da3429bd9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 22 10:56:29 minikube cri-dockerd[1446]: time="2024-05-22T10:56:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c2b27a17f12e09701d91af32afa672ab71361859439021f41ec3c4f8b8c40012/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 22 10:56:29 minikube cri-dockerd[1446]: time="2024-05-22T10:56:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e043a09b644fe7446cd69a5ba5b30c2ce5ad064a084d76241bb44e7fd8426ea8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 22 10:56:29 minikube cri-dockerd[1446]: time="2024-05-22T10:56:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3ffbdb4afbf2f0dfcfa79161a87ac8e1abc9b7dd69dbd3477084576f33bbb979/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 22 10:56:48 minikube dockerd[1221]: time="2024-05-22T10:56:48.122112426Z" level=info msg="ignoring event" container=4f618a9300566b0a6e6e225dc6b3de12a93677eb185974e4dfbd9fc6fa153cd3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 22 10:57:04 minikube dockerd[1221]: time="2024-05-22T10:57:04.259631709Z" level=info msg="ignoring event" container=9a20be143c1cecbe71ed31edbfdb2a13a35e262d23c8cdcfdeccb644881d6f7b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 22 10:57:55 minikube cri-dockerd[1446]: time="2024-05-22T10:57:55Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 22 10:58:08 minikube cri-dockerd[1446]: time="2024-05-22T10:58:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ece20b83722f6a90b8676327ff2aaf28ef4ca43349f5d0a77c7328590fc034bd/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 22 10:58:09 minikube cri-dockerd[1446]: time="2024-05-22T10:58:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c6242e2564a5b68f07f915ce27be28cdd4acd5bec63e9b7876c5606875545560/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 22 10:58:10 minikube cri-dockerd[1446]: time="2024-05-22T10:58:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/60b56c5d72ca5f15a4ee2e994d7d6e8da54ba826655da1a1a6f99d10666e2ae4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 22 11:27:04 minikube cri-dockerd[1446]: time="2024-05-22T11:27:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e20b3648852d47bf4a28db30f8d4b0515165a5c1f40e5106933fe2f538ac26e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 22 11:27:06 minikube dockerd[1221]: time="2024-05-22T11:27:06.918254039Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=57e7103e46c28998 traceID=8453441810ec3c519dd75908f0c1766b
May 22 11:27:06 minikube dockerd[1221]: time="2024-05-22T11:27:06.918332149Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 22 11:27:20 minikube dockerd[1221]: time="2024-05-22T11:27:20.767616264Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=c24d5867cd9f8c14 traceID=446d905df64152c98c3f8cf9b67ce8c1
May 22 11:27:20 minikube dockerd[1221]: time="2024-05-22T11:27:20.767695019Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 22 11:27:50 minikube dockerd[1221]: time="2024-05-22T11:27:50.968286959Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=ff75fadde67c3e48 traceID=9f080be23e940d1b03f0330b2936c3d6
May 22 11:27:50 minikube dockerd[1221]: time="2024-05-22T11:27:50.968364773Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 22 11:28:43 minikube dockerd[1221]: time="2024-05-22T11:28:43.942346719Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=27910b86dd3250e2 traceID=8337464e90c90065c63ae54a9f445f2e
May 22 11:28:43 minikube dockerd[1221]: time="2024-05-22T11:28:43.942421469Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 22 11:30:16 minikube dockerd[1221]: time="2024-05-22T11:30:16.124799381Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b821231ab26cedba traceID=02ab0559fa542b38868a58434ca8a1a0
May 22 11:30:16 minikube dockerd[1221]: time="2024-05-22T11:30:16.124944887Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 22 11:33:01 minikube dockerd[1221]: time="2024-05-22T11:33:01.862385619Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=da56bf7525b16c5c traceID=b52a2c4fd0377a76000bbde4d7881195
May 22 11:33:01 minikube dockerd[1221]: time="2024-05-22T11:33:01.862539218Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 22 11:38:11 minikube dockerd[1221]: time="2024-05-22T11:38:11.755143282Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=fd76b32e8293c002 traceID=5d08ff59800c288a1a4dad97c4e53a4d
May 22 11:38:11 minikube dockerd[1221]: time="2024-05-22T11:38:11.755349032Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 22 11:43:25 minikube dockerd[1221]: time="2024-05-22T11:43:25.998376185Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=345f2ebc6069e571 traceID=aa9b750de08f8b31cdd737ae324b78f3
May 22 11:43:25 minikube dockerd[1221]: time="2024-05-22T11:43:25.998581808Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
4b0852141a4e2       6e38f40d628db       49 minutes ago      Running             storage-provisioner       0                   60b56c5d72ca5       storage-provisioner
2dc9f1d3be419       a0bf559e280cf       49 minutes ago      Running             kube-proxy                0                   c6242e2564a5b       kube-proxy-f99ms
00a1b14074d67       cbb01a7bd410d       49 minutes ago      Running             coredns                   0                   ece20b83722f6       coredns-7db6d8ff4d-hvhwn
04454d6c73dfe       c7aad43836fa5       50 minutes ago      Running             kube-controller-manager   2                   c2b27a17f12e0       kube-controller-manager-minikube
9a20be143c1ce       c7aad43836fa5       50 minutes ago      Exited              kube-controller-manager   1                   c2b27a17f12e0       kube-controller-manager-minikube
a681abca66b30       3861cfcd7c04c       50 minutes ago      Running             etcd                      0                   3ffbdb4afbf2f       etcd-minikube
211b988ee8b9e       259c8277fcbbc       50 minutes ago      Running             kube-scheduler            0                   e043a09b644fe       kube-scheduler-minikube
18676250174a4       c42f13656d0b2       50 minutes ago      Running             kube-apiserver            0                   53362bb427ec3       kube-apiserver-minikube


==> coredns [00a1b14074d6] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:60150 - 12506 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 6.002776184s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:42621->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:41759 - 37606 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 6.001111509s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:34818->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:36497 - 42088 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 4.00162568s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:50817->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:44450 - 47690 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 2.001382023s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:43038->192.168.65.254:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:41366 - 14560 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 2.000767932s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:47808->192.168.65.254:53: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1260227015]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (22-May-2024 10:58:12.671) (total time: 21042ms):
Trace[1260227015]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21041ms (10:58:33.712)
Trace[1260227015]: [21.042247023s] [21.042247023s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1739899482]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (22-May-2024 10:58:12.671) (total time: 21042ms):
Trace[1739899482]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21041ms (10:58:33.712)
Trace[1739899482]: [21.042313395s] [21.042313395s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[873998635]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (22-May-2024 10:58:12.671) (total time: 21042ms):
Trace[873998635]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21041ms (10:58:33.712)
Trace[873998635]: [21.042298028s] [21.042298028s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] 127.0.0.1:43677 - 25114 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 2.000635866s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:49378->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:39325 - 43682 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 2.000504725s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:43858->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:47778 - 52140 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" - - 0 2.001489364s
[ERROR] plugin/errors: 2 8275872313256655708.9114902785055556927. HINFO: read udp 10.244.0.2:47534->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:54796 - 48735 "HINFO IN 8275872313256655708.9114902785055556927. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.005499421s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_22T13_57_31_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 22 May 2024 10:56:38 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 22 May 2024 11:47:20 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 22 May 2024 11:43:54 +0000   Wed, 22 May 2024 10:56:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 22 May 2024 11:43:54 +0000   Wed, 22 May 2024 10:56:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 22 May 2024 11:43:54 +0000   Wed, 22 May 2024 10:56:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 22 May 2024 11:43:54 +0000   Wed, 22 May 2024 10:56:39 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8015944Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8015944Ki
  pods:               110
System Info:
  Machine ID:                 3b696acc002249fb8417ee82946ed5da
  System UUID:                3b696acc002249fb8417ee82946ed5da
  Boot ID:                    9f6fd36a-ff4f-4f33-94ae-7b2eef9fb479
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     springboot-app-775f645bdf-hdxhx     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20m
  kube-system                 coredns-7db6d8ff4d-hvhwn            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     49m
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         50m
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         50m
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         50m
  kube-system                 kube-proxy-f99ms                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         49m
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         50m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         49m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 49m                kube-proxy       
  Normal  NodeHasSufficientMemory  51m (x8 over 51m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    51m (x8 over 51m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     51m (x7 over 51m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  51m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 50m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  50m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    50m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     50m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  50m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           49m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000943] FS-Cache: N-cookie d=0000000063fea060{9P.session} n=00000000d52b4713
[  +0.000890] FS-Cache: N-key=[10] '34323934393431333339'
[  +0.030150] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001502] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.001845] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.001288] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.003764] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001940] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.002968] WSL (4) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001183] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.002963] WSL (5) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.007519] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.011171] WSL (1) WARNING: /usr/share/zoneinfo/Africa/Cairo not found. Is the tzdata package installed?
[  +0.139207] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001797] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001817] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002699] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003428] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001746] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001892] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002135] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002447] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +3.715939] netlink: 'init': attribute type 4 has an invalid length.
[  +6.405226] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000483] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.861713] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000266] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001250] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001844] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000865] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001659] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000187] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001455] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000106] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001041] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000084] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001501] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002448] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002428] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000807] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000382] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000777] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000108] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000796] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001662] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001152] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000625] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000933] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001220] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000550] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000615] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000589] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001120] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +4.010926] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.015357] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2


==> etcd [a681abca66b3] <==
{"level":"info","ts":"2024-05-22T11:38:14.719799Z","caller":"traceutil/trace.go:171","msg":"trace[98977273] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:0; response_revision:2360; }","duration":"113.239767ms","start":"2024-05-22T11:38:14.606542Z","end":"2024-05-22T11:38:14.719782Z","steps":["trace[98977273] 'agreement among raft nodes before linearized reading'  (duration: 112.913575ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:38:30.629721Z","caller":"traceutil/trace.go:171","msg":"trace[1154973944] transaction","detail":"{read_only:false; response_revision:2374; number_of_response:1; }","duration":"139.825117ms","start":"2024-05-22T11:38:30.489862Z","end":"2024-05-22T11:38:30.629687Z","steps":["trace[1154973944] 'process raft request'  (duration: 139.146911ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:38:32.788534Z","caller":"traceutil/trace.go:171","msg":"trace[1879937061] transaction","detail":"{read_only:false; response_revision:2375; number_of_response:1; }","duration":"144.062288ms","start":"2024-05-22T11:38:32.644446Z","end":"2024-05-22T11:38:32.788509Z","steps":["trace[1879937061] 'process raft request'  (duration: 143.888843ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:38:34.937524Z","caller":"traceutil/trace.go:171","msg":"trace[303372224] transaction","detail":"{read_only:false; response_revision:2377; number_of_response:1; }","duration":"141.709035ms","start":"2024-05-22T11:38:34.795788Z","end":"2024-05-22T11:38:34.937497Z","steps":["trace[303372224] 'process raft request'  (duration: 141.49089ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:38:37.049386Z","caller":"traceutil/trace.go:171","msg":"trace[755492427] transaction","detail":"{read_only:false; response_revision:2379; number_of_response:1; }","duration":"105.927725ms","start":"2024-05-22T11:38:36.943429Z","end":"2024-05-22T11:38:37.049357Z","steps":["trace[755492427] 'process raft request'  (duration: 105.736412ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:38:41.237914Z","caller":"traceutil/trace.go:171","msg":"trace[1488723789] transaction","detail":"{read_only:false; response_revision:2383; number_of_response:1; }","duration":"105.648038ms","start":"2024-05-22T11:38:41.132228Z","end":"2024-05-22T11:38:41.237876Z","steps":["trace[1488723789] 'process raft request'  (duration: 105.242375ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:38:49.124339Z","caller":"traceutil/trace.go:171","msg":"trace[79190390] transaction","detail":"{read_only:false; response_revision:2390; number_of_response:1; }","duration":"108.138734ms","start":"2024-05-22T11:38:49.016168Z","end":"2024-05-22T11:38:49.124306Z","steps":["trace[79190390] 'process raft request'  (duration: 58.365477ms)","trace[79190390] 'compare'  (duration: 49.627795ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:38:55.689637Z","caller":"traceutil/trace.go:171","msg":"trace[1595351523] transaction","detail":"{read_only:false; response_revision:2395; number_of_response:1; }","duration":"145.657243ms","start":"2024-05-22T11:38:55.54392Z","end":"2024-05-22T11:38:55.689577Z","steps":["trace[1595351523] 'process raft request'  (duration: 145.341941ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:39:08.216012Z","caller":"traceutil/trace.go:171","msg":"trace[405203113] transaction","detail":"{read_only:false; response_revision:2405; number_of_response:1; }","duration":"198.450313ms","start":"2024-05-22T11:39:08.017534Z","end":"2024-05-22T11:39:08.215985Z","steps":["trace[405203113] 'process raft request'  (duration: 174.334773ms)","trace[405203113] 'compare'  (duration: 23.988607ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:39:14.491928Z","caller":"traceutil/trace.go:171","msg":"trace[719248170] transaction","detail":"{read_only:false; response_revision:2410; number_of_response:1; }","duration":"146.279982ms","start":"2024-05-22T11:39:14.345608Z","end":"2024-05-22T11:39:14.491888Z","steps":["trace[719248170] 'process raft request'  (duration: 145.833387ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:39:16.611891Z","caller":"traceutil/trace.go:171","msg":"trace[392332217] transaction","detail":"{read_only:false; response_revision:2411; number_of_response:1; }","duration":"107.780603ms","start":"2024-05-22T11:39:16.502676Z","end":"2024-05-22T11:39:16.610456Z","steps":["trace[392332217] 'process raft request'  (duration: 107.557288ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:39:29.014254Z","caller":"traceutil/trace.go:171","msg":"trace[1295168182] transaction","detail":"{read_only:false; response_revision:2421; number_of_response:1; }","duration":"101.144454ms","start":"2024-05-22T11:39:28.913079Z","end":"2024-05-22T11:39:29.014223Z","steps":["trace[1295168182] 'process raft request'  (duration: 100.41968ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:39:41.509306Z","caller":"traceutil/trace.go:171","msg":"trace[887584883] transaction","detail":"{read_only:false; response_revision:2431; number_of_response:1; }","duration":"100.49963ms","start":"2024-05-22T11:39:41.408762Z","end":"2024-05-22T11:39:41.509261Z","steps":["trace[887584883] 'process raft request'  (duration: 100.150746ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:40:00.195546Z","caller":"traceutil/trace.go:171","msg":"trace[1345771714] transaction","detail":"{read_only:false; response_revision:2446; number_of_response:1; }","duration":"135.77537ms","start":"2024-05-22T11:40:00.059749Z","end":"2024-05-22T11:40:00.195525Z","steps":["trace[1345771714] 'process raft request'  (duration: 135.516233ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:40:12.655872Z","caller":"traceutil/trace.go:171","msg":"trace[1496219105] transaction","detail":"{read_only:false; response_revision:2455; number_of_response:1; }","duration":"139.087384ms","start":"2024-05-22T11:40:12.516765Z","end":"2024-05-22T11:40:12.655852Z","steps":["trace[1496219105] 'process raft request'  (duration: 138.983286ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:40:16.892808Z","caller":"traceutil/trace.go:171","msg":"trace[1390764405] transaction","detail":"{read_only:false; response_revision:2458; number_of_response:1; }","duration":"118.292566ms","start":"2024-05-22T11:40:16.774479Z","end":"2024-05-22T11:40:16.892772Z","steps":["trace[1390764405] 'process raft request'  (duration: 117.579011ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:40:36.472814Z","caller":"traceutil/trace.go:171","msg":"trace[342814564] transaction","detail":"{read_only:false; response_revision:2473; number_of_response:1; }","duration":"130.724832ms","start":"2024-05-22T11:40:36.342057Z","end":"2024-05-22T11:40:36.472782Z","steps":["trace[342814564] 'process raft request'  (duration: 130.512366ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:40:37.529466Z","caller":"traceutil/trace.go:171","msg":"trace[1291933884] transaction","detail":"{read_only:false; response_revision:2474; number_of_response:1; }","duration":"105.635844ms","start":"2024-05-22T11:40:37.423801Z","end":"2024-05-22T11:40:37.529437Z","steps":["trace[1291933884] 'process raft request'  (duration: 105.463686ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:41:12.533162Z","caller":"traceutil/trace.go:171","msg":"trace[454568070] transaction","detail":"{read_only:false; response_revision:2502; number_of_response:1; }","duration":"100.954098ms","start":"2024-05-22T11:41:12.432173Z","end":"2024-05-22T11:41:12.533127Z","steps":["trace[454568070] 'process raft request'  (duration: 100.550082ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:41:18.816629Z","caller":"traceutil/trace.go:171","msg":"trace[1200012039] transaction","detail":"{read_only:false; response_revision:2506; number_of_response:1; }","duration":"183.975261ms","start":"2024-05-22T11:41:18.632622Z","end":"2024-05-22T11:41:18.816598Z","steps":["trace[1200012039] 'process raft request'  (duration: 183.813506ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:41:39.195842Z","caller":"traceutil/trace.go:171","msg":"trace[22490328] linearizableReadLoop","detail":"{readStateIndex:3084; appliedIndex:3083; }","duration":"121.869895ms","start":"2024-05-22T11:41:39.073939Z","end":"2024-05-22T11:41:39.195809Z","steps":["trace[22490328] 'read index received'  (duration: 22.797231ms)","trace[22490328] 'applied index is now lower than readState.Index'  (duration: 99.070873ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-22T11:41:39.196082Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.100898ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2024-05-22T11:41:39.19615Z","caller":"traceutil/trace.go:171","msg":"trace[94119377] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:2523; }","duration":"122.211942ms","start":"2024-05-22T11:41:39.073922Z","end":"2024-05-22T11:41:39.196134Z","steps":["trace[94119377] 'agreement among raft nodes before linearized reading'  (duration: 122.052624ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:41:39.196182Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2283}
{"level":"info","ts":"2024-05-22T11:41:39.196382Z","caller":"traceutil/trace.go:171","msg":"trace[1524724479] compact","detail":"{revision:2283; response_revision:2523; }","duration":"123.279164ms","start":"2024-05-22T11:41:39.073092Z","end":"2024-05-22T11:41:39.196371Z","steps":["trace[1524724479] 'check and update compact revision'  (duration: 98.892407ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:41:39.267623Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2283,"took":"71.01973ms","hash":160163965,"current-db-size-bytes":1540096,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1032192,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-05-22T11:41:39.267755Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":160163965,"revision":2283,"compact-revision":2045}
{"level":"info","ts":"2024-05-22T11:41:45.683952Z","caller":"traceutil/trace.go:171","msg":"trace[1915747723] transaction","detail":"{read_only:false; response_revision:2528; number_of_response:1; }","duration":"102.028253ms","start":"2024-05-22T11:41:45.581896Z","end":"2024-05-22T11:41:45.683924Z","steps":["trace[1915747723] 'process raft request'  (duration: 101.84172ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:41:58.044045Z","caller":"traceutil/trace.go:171","msg":"trace[797317412] transaction","detail":"{read_only:false; response_revision:2537; number_of_response:1; }","duration":"136.619783ms","start":"2024-05-22T11:41:57.907388Z","end":"2024-05-22T11:41:58.044008Z","steps":["trace[797317412] 'process raft request'  (duration: 136.423782ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:42:20.856204Z","caller":"traceutil/trace.go:171","msg":"trace[1652867354] transaction","detail":"{read_only:false; response_revision:2557; number_of_response:1; }","duration":"102.414188ms","start":"2024-05-22T11:42:20.753753Z","end":"2024-05-22T11:42:20.856167Z","steps":["trace[1652867354] 'process raft request'  (duration: 102.228147ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-22T11:42:34.680533Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.820287ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029344621502298 > lease_revoke:<id:70cc8f9ff21d9f21>","response":"size:29"}
{"level":"info","ts":"2024-05-22T11:43:20.573361Z","caller":"traceutil/trace.go:171","msg":"trace[707829848] transaction","detail":"{read_only:false; response_revision:2603; number_of_response:1; }","duration":"105.922308ms","start":"2024-05-22T11:43:20.467154Z","end":"2024-05-22T11:43:20.573077Z","steps":["trace[707829848] 'process raft request'  (duration: 105.725859ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:43:30.851685Z","caller":"traceutil/trace.go:171","msg":"trace[71445077] transaction","detail":"{read_only:false; response_revision:2611; number_of_response:1; }","duration":"125.580589ms","start":"2024-05-22T11:43:30.726085Z","end":"2024-05-22T11:43:30.851666Z","steps":["trace[71445077] 'process raft request'  (duration: 125.43739ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:43:51.508144Z","caller":"traceutil/trace.go:171","msg":"trace[687519005] transaction","detail":"{read_only:false; response_revision:2628; number_of_response:1; }","duration":"124.428905ms","start":"2024-05-22T11:43:51.383681Z","end":"2024-05-22T11:43:51.50811Z","steps":["trace[687519005] 'process raft request'  (duration: 124.224352ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:43:51.780221Z","caller":"traceutil/trace.go:171","msg":"trace[1254106550] transaction","detail":"{read_only:false; response_revision:2630; number_of_response:1; }","duration":"132.251281ms","start":"2024-05-22T11:43:51.647932Z","end":"2024-05-22T11:43:51.780184Z","steps":["trace[1254106550] 'process raft request'  (duration: 83.506274ms)","trace[1254106550] 'compare'  (duration: 48.625716ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:44:01.998026Z","caller":"traceutil/trace.go:171","msg":"trace[1098409064] transaction","detail":"{read_only:false; response_revision:2639; number_of_response:1; }","duration":"107.364212ms","start":"2024-05-22T11:44:01.890618Z","end":"2024-05-22T11:44:01.997982Z","steps":["trace[1098409064] 'process raft request'  (duration: 57.860954ms)","trace[1098409064] 'compare'  (duration: 49.293288ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:44:55.7889Z","caller":"traceutil/trace.go:171","msg":"trace[21724413] transaction","detail":"{read_only:false; response_revision:2680; number_of_response:1; }","duration":"148.883897ms","start":"2024-05-22T11:44:55.639989Z","end":"2024-05-22T11:44:55.788873Z","steps":["trace[21724413] 'process raft request'  (duration: 148.450337ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:45:02.005155Z","caller":"traceutil/trace.go:171","msg":"trace[971480374] transaction","detail":"{read_only:false; response_revision:2685; number_of_response:1; }","duration":"137.440854ms","start":"2024-05-22T11:45:01.867478Z","end":"2024-05-22T11:45:02.004919Z","steps":["trace[971480374] 'process raft request'  (duration: 137.225824ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:45:27.053887Z","caller":"traceutil/trace.go:171","msg":"trace[115363841] transaction","detail":"{read_only:false; response_revision:2705; number_of_response:1; }","duration":"120.62351ms","start":"2024-05-22T11:45:26.933219Z","end":"2024-05-22T11:45:27.053842Z","steps":["trace[115363841] 'process raft request'  (duration: 120.357906ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:45:34.307915Z","caller":"traceutil/trace.go:171","msg":"trace[1191787617] transaction","detail":"{read_only:false; response_revision:2710; number_of_response:1; }","duration":"107.995185ms","start":"2024-05-22T11:45:34.199881Z","end":"2024-05-22T11:45:34.307876Z","steps":["trace[1191787617] 'process raft request'  (duration: 107.650652ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:45:37.071912Z","caller":"traceutil/trace.go:171","msg":"trace[28349404] transaction","detail":"{read_only:false; response_revision:2712; number_of_response:1; }","duration":"156.569905ms","start":"2024-05-22T11:45:36.915316Z","end":"2024-05-22T11:45:37.071886Z","steps":["trace[28349404] 'process raft request'  (duration: 156.346234ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:45:59.12888Z","caller":"traceutil/trace.go:171","msg":"trace[288906099] transaction","detail":"{read_only:false; response_revision:2730; number_of_response:1; }","duration":"132.638538ms","start":"2024-05-22T11:45:58.996204Z","end":"2024-05-22T11:45:59.128842Z","steps":["trace[288906099] 'process raft request'  (duration: 83.153121ms)","trace[288906099] 'compare'  (duration: 49.220743ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:46:08.060686Z","caller":"traceutil/trace.go:171","msg":"trace[396121100] transaction","detail":"{read_only:false; response_revision:2736; number_of_response:1; }","duration":"129.228264ms","start":"2024-05-22T11:46:07.931436Z","end":"2024-05-22T11:46:08.060664Z","steps":["trace[396121100] 'process raft request'  (duration: 129.060268ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:08.094905Z","caller":"traceutil/trace.go:171","msg":"trace[378553420] transaction","detail":"{read_only:false; response_revision:2737; number_of_response:1; }","duration":"119.995689ms","start":"2024-05-22T11:46:07.9748Z","end":"2024-05-22T11:46:08.094796Z","steps":["trace[378553420] 'process raft request'  (duration: 119.897201ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:14.284491Z","caller":"traceutil/trace.go:171","msg":"trace[655933275] transaction","detail":"{read_only:false; response_revision:2741; number_of_response:1; }","duration":"114.84268ms","start":"2024-05-22T11:46:14.169614Z","end":"2024-05-22T11:46:14.284457Z","steps":["trace[655933275] 'process raft request'  (duration: 114.687929ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:25.677668Z","caller":"traceutil/trace.go:171","msg":"trace[170504999] transaction","detail":"{read_only:false; response_revision:2750; number_of_response:1; }","duration":"247.238954ms","start":"2024-05-22T11:46:25.430395Z","end":"2024-05-22T11:46:25.677634Z","steps":["trace[170504999] 'process raft request'  (duration: 246.977662ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:37.20507Z","caller":"traceutil/trace.go:171","msg":"trace[49975164] transaction","detail":"{read_only:false; response_revision:2759; number_of_response:1; }","duration":"157.85827ms","start":"2024-05-22T11:46:37.047178Z","end":"2024-05-22T11:46:37.205036Z","steps":["trace[49975164] 'process raft request'  (duration: 157.598199ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:39.107169Z","caller":"traceutil/trace.go:171","msg":"trace[919961198] transaction","detail":"{read_only:false; response_revision:2760; number_of_response:1; }","duration":"143.39983ms","start":"2024-05-22T11:46:38.963715Z","end":"2024-05-22T11:46:39.107115Z","steps":["trace[919961198] 'process raft request'  (duration: 118.56479ms)","trace[919961198] 'compare'  (duration: 24.695911ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:46:39.333121Z","caller":"traceutil/trace.go:171","msg":"trace[1477790247] linearizableReadLoop","detail":"{readStateIndex:3385; appliedIndex:3384; }","duration":"123.152639ms","start":"2024-05-22T11:46:39.209933Z","end":"2024-05-22T11:46:39.333086Z","steps":["trace[1477790247] 'read index received'  (duration: 24.326858ms)","trace[1477790247] 'applied index is now lower than readState.Index'  (duration: 98.823793ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-22T11:46:39.333403Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.428661ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:601"}
{"level":"info","ts":"2024-05-22T11:46:39.333426Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2523}
{"level":"info","ts":"2024-05-22T11:46:39.333668Z","caller":"traceutil/trace.go:171","msg":"trace[397926745] compact","detail":"{revision:2523; response_revision:2762; }","duration":"132.987401ms","start":"2024-05-22T11:46:39.200664Z","end":"2024-05-22T11:46:39.333651Z","steps":["trace[397926745] 'process raft request'  (duration: 33.625619ms)","trace[397926745] 'check and update compact revision'  (duration: 98.586491ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:46:39.333478Z","caller":"traceutil/trace.go:171","msg":"trace[1930091071] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:2762; }","duration":"123.550494ms","start":"2024-05-22T11:46:39.209906Z","end":"2024-05-22T11:46:39.333456Z","steps":["trace[1930091071] 'agreement among raft nodes before linearized reading'  (duration: 123.380576ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:39.418456Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2523,"took":"84.443024ms","hash":275517041,"current-db-size-bytes":1540096,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1036288,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-05-22T11:46:39.418659Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":275517041,"revision":2523,"compact-revision":2283}
{"level":"info","ts":"2024-05-22T11:46:45.62814Z","caller":"traceutil/trace.go:171","msg":"trace[1973077989] transaction","detail":"{read_only:false; response_revision:2766; number_of_response:1; }","duration":"119.434189ms","start":"2024-05-22T11:46:45.508694Z","end":"2024-05-22T11:46:45.628128Z","steps":["trace[1973077989] 'process raft request'  (duration: 119.314895ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:49.099538Z","caller":"traceutil/trace.go:171","msg":"trace[419677409] transaction","detail":"{read_only:false; response_revision:2769; number_of_response:1; }","duration":"104.305291ms","start":"2024-05-22T11:46:48.995211Z","end":"2024-05-22T11:46:49.099516Z","steps":["trace[419677409] 'process raft request'  (duration: 54.76347ms)","trace[419677409] 'compare'  (duration: 49.458698ms)"],"step_count":2}
{"level":"info","ts":"2024-05-22T11:46:51.834184Z","caller":"traceutil/trace.go:171","msg":"trace[1043072835] transaction","detail":"{read_only:false; response_revision:2772; number_of_response:1; }","duration":"107.44018ms","start":"2024-05-22T11:46:51.726715Z","end":"2024-05-22T11:46:51.834155Z","steps":["trace[1043072835] 'process raft request'  (duration: 107.142581ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:46:58.015665Z","caller":"traceutil/trace.go:171","msg":"trace[581035002] transaction","detail":"{read_only:false; response_revision:2776; number_of_response:1; }","duration":"104.806374ms","start":"2024-05-22T11:46:57.910826Z","end":"2024-05-22T11:46:58.015632Z","steps":["trace[581035002] 'process raft request'  (duration: 104.295803ms)"],"step_count":1}
{"level":"info","ts":"2024-05-22T11:47:08.370985Z","caller":"traceutil/trace.go:171","msg":"trace[297387828] transaction","detail":"{read_only:false; response_revision:2784; number_of_response:1; }","duration":"111.52937ms","start":"2024-05-22T11:47:08.259403Z","end":"2024-05-22T11:47:08.370932Z","steps":["trace[297387828] 'process raft request'  (duration: 111.464955ms)"],"step_count":1}


==> kernel <==
 11:47:25 up  4:16,  0 users,  load average: 0.45, 0.39, 0.43
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [18676250174a] <==
Trace[767850180]: ["GuaranteedUpdate etcd3" audit-id:e4030185-8082-4bf8-8af2-358ee0318325,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1747ms (11:07:19.627)
Trace[767850180]:  ---"Txn call completed" 1746ms (11:07:21.374)]
Trace[767850180]: [1.747399346s] [1.747399346s] END
I0522 11:10:25.125185       1 trace.go:236] Trace[1012395515]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:174df4c7-917e-41e5-8894-b557c12efafc,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (22-May-2024 11:10:24.534) (total time: 590ms):
Trace[1012395515]: ["GuaranteedUpdate etcd3" audit-id:174df4c7-917e-41e5-8894-b557c12efafc,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 590ms (11:10:24.534)
Trace[1012395515]:  ---"Txn call completed" 589ms (11:10:25.124)]
Trace[1012395515]: [590.313319ms] [590.313319ms] END
I0522 11:10:45.884281       1 trace.go:236] Trace[2122562390]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5456fbb1-0a47-44c2-8e1d-d3e193187b20,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (22-May-2024 11:10:45.360) (total time: 523ms):
Trace[2122562390]: ["GuaranteedUpdate etcd3" audit-id:5456fbb1-0a47-44c2-8e1d-d3e193187b20,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 523ms (11:10:45.360)
Trace[2122562390]:  ---"Txn call completed" 522ms (11:10:45.884)]
Trace[2122562390]: [523.391956ms] [523.391956ms] END
I0522 11:12:47.907327       1 trace.go:236] Trace[1243196406]: "Update" accept:application/json, */*,audit-id:7f88488b-37ae-42d6-9f1d-524a56c05a3f,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (22-May-2024 11:12:47.351) (total time: 555ms):
Trace[1243196406]: ["GuaranteedUpdate etcd3" audit-id:7f88488b-37ae-42d6-9f1d-524a56c05a3f,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 555ms (11:12:47.351)
Trace[1243196406]:  ---"Txn call completed" 554ms (11:12:47.907)]
Trace[1243196406]: [555.564919ms] [555.564919ms] END
I0522 11:14:39.623633       1 trace.go:236] Trace[1572941050]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (22-May-2024 11:14:38.944) (total time: 679ms):
Trace[1572941050]: ---"initial value restored" 221ms (11:14:39.165)
Trace[1572941050]: ---"Transaction prepared" 194ms (11:14:39.359)
Trace[1572941050]: ---"Txn call completed" 263ms (11:14:39.623)
Trace[1572941050]: [679.320241ms] [679.320241ms] END
I0522 11:17:29.551540       1 trace.go:236] Trace[671322045]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (22-May-2024 11:17:28.944) (total time: 607ms):
Trace[671322045]: ---"Transaction prepared" 376ms (11:17:29.321)
Trace[671322045]: ---"Txn call completed" 229ms (11:17:29.551)
Trace[671322045]: [607.16419ms] [607.16419ms] END
I0522 11:17:50.382096       1 trace.go:236] Trace[231233910]: "Update" accept:application/json, */*,audit-id:5143af22-3afc-45ae-8942-c9221cf7469c,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (22-May-2024 11:17:49.813) (total time: 568ms):
Trace[231233910]: ["GuaranteedUpdate etcd3" audit-id:5143af22-3afc-45ae-8942-c9221cf7469c,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 567ms (11:17:49.814)
Trace[231233910]:  ---"Txn call completed" 567ms (11:17:50.381)]
Trace[231233910]: [568.128015ms] [568.128015ms] END
I0522 11:18:59.933397       1 trace.go:236] Trace[688206646]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (22-May-2024 11:18:58.945) (total time: 987ms):
Trace[688206646]: ---"Transaction prepared" 684ms (11:18:59.631)
Trace[688206646]: ---"Txn call completed" 301ms (11:18:59.933)
Trace[688206646]: [987.556685ms] [987.556685ms] END
I0522 11:19:34.802902       1 trace.go:236] Trace[768311710]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:c8b91a61-acf4-4755-9fb5-776a9fbb79fd,client:192.168.49.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (22-May-2024 11:19:34.093) (total time: 709ms):
Trace[768311710]: ["GuaranteedUpdate etcd3" audit-id:c8b91a61-acf4-4755-9fb5-776a9fbb79fd,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 709ms (11:19:34.093)
Trace[768311710]:  ---"Txn call completed" 708ms (11:19:34.802)]
Trace[768311710]: [709.410457ms] [709.410457ms] END
I0522 11:19:35.020113       1 trace.go:236] Trace[197865465]: "Get" accept:application/json, */*,audit-id:56d9fdd8-4fa9-46b8-a96e-290526e1dd26,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (22-May-2024 11:19:34.503) (total time: 516ms):
Trace[197865465]: ---"About to write a response" 516ms (11:19:35.020)
Trace[197865465]: [516.484439ms] [516.484439ms] END
I0522 11:20:07.540265       1 trace.go:236] Trace[137835942]: "Update" accept:application/json, */*,audit-id:23490c54-b27c-4f61-895d-76d3a1316f46,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (22-May-2024 11:20:07.018) (total time: 521ms):
Trace[137835942]: ["GuaranteedUpdate etcd3" audit-id:23490c54-b27c-4f61-895d-76d3a1316f46,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 521ms (11:20:07.019)
Trace[137835942]:  ---"Txn call completed" 520ms (11:20:07.540)]
Trace[137835942]: [521.454081ms] [521.454081ms] END
I0522 11:20:29.511181       1 trace.go:236] Trace[1966414127]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (22-May-2024 11:20:28.947) (total time: 563ms):
Trace[1966414127]: ---"initial value restored" 117ms (11:20:29.065)
Trace[1966414127]: ---"Transaction prepared" 354ms (11:20:29.419)
Trace[1966414127]: ---"Txn call completed" 91ms (11:20:29.511)
Trace[1966414127]: [563.484752ms] [563.484752ms] END
I0522 11:20:59.517914       1 trace.go:236] Trace[659778287]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (22-May-2024 11:20:58.948) (total time: 569ms):
Trace[659778287]: ---"Transaction prepared" 255ms (11:20:59.204)
Trace[659778287]: ---"Txn call completed" 313ms (11:20:59.517)
Trace[659778287]: [569.509638ms] [569.509638ms] END
I0522 11:22:04.801124       1 trace.go:236] Trace[377456959]: "Get" accept:application/json, */*,audit-id:e3faf2fb-2b43-404a-9785-c605293a6b20,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (22-May-2024 11:22:04.251) (total time: 549ms):
Trace[377456959]: ---"About to write a response" 549ms (11:22:04.801)
Trace[377456959]: [549.406239ms] [549.406239ms] END
I0522 11:27:12.281523       1 alloc.go:330] "allocated clusterIPs" service="default/springboot-app-service" clusterIPs={"IPv4":"10.98.209.112"}
I0522 11:27:59.590957       1 trace.go:236] Trace[1331538574]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (22-May-2024 11:27:58.949) (total time: 640ms):
Trace[1331538574]: ---"Transaction prepared" 450ms (11:27:59.401)
Trace[1331538574]: ---"Txn call completed" 189ms (11:27:59.590)
Trace[1331538574]: [640.927112ms] [640.927112ms] END


==> kube-controller-manager [04454d6c73df] <==
I0522 10:57:51.124864       1 shared_informer.go:320] Caches are synced for job
I0522 10:57:51.127213       1 shared_informer.go:320] Caches are synced for disruption
I0522 10:57:51.128416       1 shared_informer.go:320] Caches are synced for ReplicationController
I0522 10:57:51.129608       1 shared_informer.go:320] Caches are synced for PVC protection
I0522 10:57:51.152496       1 shared_informer.go:320] Caches are synced for ephemeral
I0522 10:57:51.153929       1 shared_informer.go:320] Caches are synced for expand
I0522 10:57:51.156550       1 shared_informer.go:320] Caches are synced for service account
I0522 10:57:51.161758       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0522 10:57:51.165423       1 shared_informer.go:320] Caches are synced for HPA
I0522 10:57:51.165469       1 shared_informer.go:320] Caches are synced for deployment
I0522 10:57:51.211962       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0522 10:57:51.220449       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0522 10:57:51.230222       1 shared_informer.go:320] Caches are synced for taint
I0522 10:57:51.230398       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0522 10:57:51.230883       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0522 10:57:51.231067       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0522 10:57:51.242395       1 shared_informer.go:320] Caches are synced for persistent volume
I0522 10:57:51.254071       1 shared_informer.go:320] Caches are synced for daemon sets
I0522 10:57:51.269060       1 shared_informer.go:320] Caches are synced for attach detach
I0522 10:57:51.274804       1 shared_informer.go:320] Caches are synced for TTL
I0522 10:57:51.279225       1 shared_informer.go:320] Caches are synced for node
I0522 10:57:51.279275       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0522 10:57:51.279286       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0522 10:57:51.279289       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0522 10:57:51.279293       1 shared_informer.go:320] Caches are synced for cidrallocator
I0522 10:57:51.292372       1 shared_informer.go:320] Caches are synced for GC
I0522 10:57:51.301585       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0522 10:57:51.360790       1 shared_informer.go:320] Caches are synced for resource quota
I0522 10:57:51.384611       1 shared_informer.go:320] Caches are synced for resource quota
I0522 10:57:51.764094       1 shared_informer.go:320] Caches are synced for garbage collector
I0522 10:57:51.764196       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0522 10:57:51.767572       1 shared_informer.go:320] Caches are synced for garbage collector
I0522 10:57:52.635430       1 range_allocator.go:381] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0522 10:57:57.883846       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="4.638028928s"
I0522 10:57:58.372766       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="488.849056ms"
I0522 10:57:58.372876       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="66.712µs"
I0522 10:57:59.194207       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="46.272µs"
I0522 10:58:16.090029       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="65.875µs"
I0522 10:58:40.503041       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="745.425268ms"
I0522 10:58:40.503255       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="66.786µs"
I0522 11:27:02.080855       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="700.830452ms"
I0522 11:27:02.294976       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="213.905072ms"
I0522 11:27:02.295539       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="430.142µs"
I0522 11:27:02.297987       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="36.372µs"
I0522 11:27:07.816327       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="40.08µs"
I0522 11:27:18.841577       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="40.112µs"
I0522 11:27:33.815339       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="42.563µs"
I0522 11:27:48.918113       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="239.544µs"
I0522 11:28:03.867043       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="85.297µs"
I0522 11:28:14.859589       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="180.058µs"
I0522 11:28:55.856529       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="92.364µs"
I0522 11:29:10.785464       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="68.373µs"
I0522 11:30:29.852412       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="129.996µs"
I0522 11:30:42.756522       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="180.356µs"
I0522 11:33:12.754867       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="93.275µs"
I0522 11:33:24.760533       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="39.404µs"
I0522 11:38:23.733853       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="66.853µs"
I0522 11:38:35.729129       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="55.983µs"
I0522 11:43:39.719261       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="126µs"
I0522 11:43:51.782282       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/springboot-app-775f645bdf" duration="78.559µs"


==> kube-controller-manager [9a20be143c1c] <==
I0522 10:56:53.883150       1 serving.go:380] Generated self-signed cert in-memory
I0522 10:56:54.227120       1 controllermanager.go:189] "Starting" version="v1.30.0"
I0522 10:56:54.227158       1 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0522 10:56:54.228497       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0522 10:56:54.228582       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0522 10:56:54.228580       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0522 10:56:54.228630       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E0522 10:57:04.236300       1 controllermanager.go:234] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-status-available-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-proxy [2dc9f1d3be41] <==
I0522 10:58:14.370353       1 server_linux.go:69] "Using iptables proxy"
I0522 10:58:14.446368       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0522 10:58:14.465201       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0522 10:58:14.465252       1 server_linux.go:165] "Using iptables Proxier"
I0522 10:58:14.467090       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0522 10:58:14.467125       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0522 10:58:14.467143       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0522 10:58:14.467480       1 server.go:872] "Version info" version="v1.30.0"
I0522 10:58:14.467531       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0522 10:58:14.468423       1 config.go:192] "Starting service config controller"
I0522 10:58:14.468460       1 config.go:101] "Starting endpoint slice config controller"
I0522 10:58:14.468473       1 shared_informer.go:313] Waiting for caches to sync for service config
I0522 10:58:14.468473       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0522 10:58:14.468505       1 config.go:319] "Starting node config controller"
I0522 10:58:14.468510       1 shared_informer.go:313] Waiting for caches to sync for node config
I0522 10:58:14.568806       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0522 10:58:14.568833       1 shared_informer.go:320] Caches are synced for node config
I0522 10:58:14.568882       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [211b988ee8b9] <==
E0522 10:56:44.432059       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0522 10:56:44.837347       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0522 10:56:44.837443       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0522 10:56:45.255878       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:45.255927       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:45.449813       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0522 10:56:45.449863       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0522 10:56:45.496961       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:45.497062       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:45.688825       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0522 10:56:45.688886       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0522 10:56:46.111757       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0522 10:56:46.111814       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0522 10:56:46.250785       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0522 10:56:46.250835       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0522 10:56:46.563064       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0522 10:56:46.563153       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0522 10:56:47.240543       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:47.240591       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:47.998744       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:47.998798       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:48.163844       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0522 10:56:48.163894       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0522 10:56:48.548072       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0522 10:56:48.548143       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0522 10:56:48.652586       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0522 10:56:48.652634       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0522 10:56:48.656017       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0522 10:56:48.656067       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0522 10:56:51.757912       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0522 10:56:51.757962       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0522 10:56:51.935423       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0522 10:56:51.935471       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0522 10:56:52.865004       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0522 10:56:52.865071       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0522 10:56:54.083641       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0522 10:56:54.083707       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0522 10:56:54.878484       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0522 10:56:54.878535       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0522 10:56:55.238247       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:55.238376       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:55.413190       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0522 10:56:55.413244       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0522 10:56:55.583055       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0522 10:56:55.583191       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0522 10:56:55.641348       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0522 10:56:55.641439       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0522 10:56:56.311811       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:56.311902       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:56.802494       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0522 10:56:56.802543       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0522 10:56:57.309992       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0522 10:56:57.310143       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0522 10:56:57.782555       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:57.782611       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:59.088497       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0522 10:56:59.088565       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0522 10:56:59.345145       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0522 10:56:59.345192       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
I0522 10:57:19.809808       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 22 11:35:50 minikube kubelet[2641]: E0522 11:35:50.681355    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:36:01 minikube kubelet[2641]: E0522 11:36:01.682341    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:36:12 minikube kubelet[2641]: E0522 11:36:12.682881    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:36:24 minikube kubelet[2641]: E0522 11:36:24.681447    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:36:35 minikube kubelet[2641]: E0522 11:36:35.680504    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:36:46 minikube kubelet[2641]: E0522 11:36:46.680925    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:36:58 minikube kubelet[2641]: E0522 11:36:58.673866    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:37:11 minikube kubelet[2641]: E0522 11:37:11.676816    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:37:24 minikube kubelet[2641]: E0522 11:37:24.677685    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:37:39 minikube kubelet[2641]: E0522 11:37:39.673057    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:37:54 minikube kubelet[2641]: E0522 11:37:54.674259    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:38:11 minikube kubelet[2641]: E0522 11:38:11.820863    2641 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="indiegogo-springboot-app:latest"
May 22 11:38:11 minikube kubelet[2641]: E0522 11:38:11.820939    2641 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="indiegogo-springboot-app:latest"
May 22 11:38:11 minikube kubelet[2641]: E0522 11:38:11.821025    2641 kuberuntime_manager.go:1256] container &Container{Name:springboot-app,Image:indiegogo-springboot-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwn4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-775f645bdf-hdxhx_default(81bdb980-dc52-4459-87ea-b2d31ef63835): ErrImagePull: Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 22 11:38:11 minikube kubelet[2641]: E0522 11:38:11.821051    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ErrImagePull: \"Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:38:23 minikube kubelet[2641]: E0522 11:38:23.668824    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:38:35 minikube kubelet[2641]: E0522 11:38:35.669145    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:38:47 minikube kubelet[2641]: E0522 11:38:47.665890    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:39:02 minikube kubelet[2641]: E0522 11:39:02.662687    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:39:13 minikube kubelet[2641]: E0522 11:39:13.665290    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:39:27 minikube kubelet[2641]: E0522 11:39:27.661653    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:39:41 minikube kubelet[2641]: E0522 11:39:41.668565    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:39:55 minikube kubelet[2641]: E0522 11:39:55.662446    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:40:08 minikube kubelet[2641]: E0522 11:40:08.657585    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:40:20 minikube kubelet[2641]: E0522 11:40:20.659288    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:40:31 minikube kubelet[2641]: E0522 11:40:31.656356    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:40:44 minikube kubelet[2641]: E0522 11:40:44.655124    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:40:58 minikube kubelet[2641]: E0522 11:40:58.652535    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:41:11 minikube kubelet[2641]: E0522 11:41:11.653550    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:41:22 minikube kubelet[2641]: E0522 11:41:22.652399    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:41:34 minikube kubelet[2641]: E0522 11:41:34.652731    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:41:49 minikube kubelet[2641]: E0522 11:41:49.653096    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:42:01 minikube kubelet[2641]: E0522 11:42:01.647904    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:42:15 minikube kubelet[2641]: E0522 11:42:15.647203    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:42:28 minikube kubelet[2641]: E0522 11:42:28.647901    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:42:42 minikube kubelet[2641]: E0522 11:42:42.647655    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:42:56 minikube kubelet[2641]: E0522 11:42:56.640250    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:43:10 minikube kubelet[2641]: E0522 11:43:10.642313    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:43:26 minikube kubelet[2641]: E0522 11:43:26.076911    2641 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="indiegogo-springboot-app:latest"
May 22 11:43:26 minikube kubelet[2641]: E0522 11:43:26.077041    2641 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="indiegogo-springboot-app:latest"
May 22 11:43:26 minikube kubelet[2641]: E0522 11:43:26.077215    2641 kuberuntime_manager.go:1256] container &Container{Name:springboot-app,Image:indiegogo-springboot-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwn4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-775f645bdf-hdxhx_default(81bdb980-dc52-4459-87ea-b2d31ef63835): ErrImagePull: Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 22 11:43:26 minikube kubelet[2641]: E0522 11:43:26.077259    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ErrImagePull: \"Error response from daemon: pull access denied for indiegogo-springboot-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:43:39 minikube kubelet[2641]: E0522 11:43:39.641267    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:43:51 minikube kubelet[2641]: E0522 11:43:51.640253    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:44:03 minikube kubelet[2641]: E0522 11:44:03.635620    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:44:16 minikube kubelet[2641]: E0522 11:44:16.639246    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:44:30 minikube kubelet[2641]: E0522 11:44:30.636249    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:44:44 minikube kubelet[2641]: E0522 11:44:44.635547    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:44:57 minikube kubelet[2641]: E0522 11:44:57.634179    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:45:08 minikube kubelet[2641]: E0522 11:45:08.632321    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:45:19 minikube kubelet[2641]: E0522 11:45:19.632808    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:45:30 minikube kubelet[2641]: E0522 11:45:30.631928    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:45:45 minikube kubelet[2641]: E0522 11:45:45.629363    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:46:00 minikube kubelet[2641]: E0522 11:46:00.628846    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:46:11 minikube kubelet[2641]: E0522 11:46:11.629238    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:46:24 minikube kubelet[2641]: E0522 11:46:24.629374    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:46:38 minikube kubelet[2641]: E0522 11:46:38.625900    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:46:51 minikube kubelet[2641]: E0522 11:46:51.624053    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:47:04 minikube kubelet[2641]: E0522 11:47:04.622093    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"
May 22 11:47:18 minikube kubelet[2641]: E0522 11:47:18.621934    2641 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app\" with ImagePullBackOff: \"Back-off pulling image \\\"indiegogo-springboot-app:latest\\\"\"" pod="default/springboot-app-775f645bdf-hdxhx" podUID="81bdb980-dc52-4459-87ea-b2d31ef63835"


==> storage-provisioner [4b0852141a4e] <==
I0522 10:58:15.678858       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0522 10:58:15.686938       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0522 10:58:15.687029       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0522 10:58:16.483048       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0522 10:58:16.483336       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_9bcce2fd-1ab3-4ba0-88d8-03c88c875985!
I0522 10:58:16.483603       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e8ede760-ba6e-4d7b-a15f-09e4ebd91747", APIVersion:"v1", ResourceVersion:"444", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_9bcce2fd-1ab3-4ba0-88d8-03c88c875985 became leader
I0522 10:58:16.583631       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_9bcce2fd-1ab3-4ba0-88d8-03c88c875985!

